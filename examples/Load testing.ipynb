{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test of an endpoint\n",
    "\n",
    "Load testing is a critical performance evaluation technique that simulates real-world usage patterns on a large language model endpoint. LLMeter helps measuring response times\n",
    "and evaluate system throughput by varying the number of concurrent client requests. In this notebook we explore two patterns: fixed prompt and representative prompts load testing. \n",
    "\n",
    "For this load testing example we'll use [Amazon Bedrock](https://aws.amazon.com/bedrock/). The results will be a function of the selected [text generation model](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) selected, as well as the [throughput quotas](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html) of the account.  \n",
    "The result of this notebook can also be used to analyze and compare endpoints, as demonstrated in [TPOT_comparison.ipynb](TPOT_comparison.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "In this section we select the model we will load test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmeter.endpoints.bedrock import BedrockConverseStream\n",
    "from llmeter.experiments import LoadTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = None  # <-- Choose a Bedrock model ID from the above-linked list that supports Converse API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create an LLMeter endpoint to manage access to the model. We'll be using a streaming endpoint, so we will also have information about the time to first token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = BedrockConverseStream(model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test with fixed payload\n",
    "\n",
    "We start by testing the endpoint by sending the same prompt consecutively, and repeat that different numbers of concurrent clients. \n",
    "Running a load test with a fixed payload allows us to compare endpoints using dimensions like _time to first token_ (TTFT), and it's also a convenient way to observe the throughput for input and output tokens, but might not be representative of the request throughput for a realistic workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = endpoint.create_payload(\n",
    "    \"this is a test request, write a short poem, then translate it to italian\",\n",
    ")\n",
    "payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the load test by indicating the sequence of clients to test. The result will also be saved in the output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test = LoadTest(\n",
    "    endpoint=endpoint,\n",
    "    payload=payload,\n",
    "    sequence_of_clients=[1, 5, 10],\n",
    "    output_path=f\"outputs/{endpoint.model_id}/load_test\",\n",
    "    min_requests_per_run=30,\n",
    "    min_requests_per_client=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test_results = await load_test.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "We can now visualize the results, using the convenience method `plot_results()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = load_test_results.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to compare this load test with another one, take note of the output path, that's where the result is stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test_results.output_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
