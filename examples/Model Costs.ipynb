{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Costs with LLMeter\n",
    "\n",
    "This notebook introduces how to use LLMeter's `CostModel` callback to estimate costs and factor these in to your comparisons between different LLMs and solution configurations.\n",
    "\n",
    "If you're new to LLMeter, you may find it helpful to follow through one of the introductory \"LLMeter with...\" notebooks first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "At a high level, to model costs with LLMeter you'll:\n",
    "1. Start by building up a cost model in line with whatever \"dimensions\" of charges are applicable for your chosen FM provider (such as charges per token, infrastructure charges for endpoint uptime, and so on)\n",
    "2. Include this model as a callback when running an LLMeter Run or Experiment, **or** calculate pricing against a previous run for which the callback wasn't enabled.\n",
    "3. Explore the request-level and run-level cost estimates calculated by your model, to help evaluate and compare FMs.\n",
    "\n",
    "> ⚠️ **In important warning: Pricing is complicated!**\n",
    ">\n",
    "> In general, many factors can affect the final charges you might incur when using Foundation Models or other Cloud services - and make comparing different types of hosting services more complicated.\n",
    ">\n",
    "> For example: services may offer volume discounts, reserved-capacity discounts, private pricing agreements, tiered pricing, or even free tiers, in which case the marginal costs of deploying a new use-case may depend on other workloads you're already committed to. Additional factors like networking and data transfer charges, gateways or other solution components may also contribute to pricing in complex ways.\n",
    ">\n",
    "> **You are ultimately responsible for understanding your cost structure.** Even in cases where LLMeter provides examples or utilities that attempt to simplify modelling the costs of endpoint types we natively support, we cannot guarantee these will be authoritative estimates *or* that they capture all the nuances of costing in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "First, ensure your environment has LLMeter installed and import the key components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llmeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from llmeter.callbacks import CostModel\n",
    "from llmeter.callbacks.cost import CalculatedCostWithDimensions, dimensions\n",
    "from llmeter.endpoints.bedrock import BedrockConverseStream\n",
    "from llmeter.endpoints.sagemaker import SageMakerEndpoint\n",
    "from llmeter.experiments import LoadTest  # Example of a higher-level \"experiment\"\n",
    "from llmeter.results import InvocationResponse, Result\n",
    "from llmeter.runner import Runner  # Low-level test runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request-based pricing example (with Amazon Bedrock)\n",
    "\n",
    "For many popular \"as-a-service\" Foundation Model APIs (like OpenAI, Anthropic, or Amazon Bedrock Converse), the major components of pricing center around the **number of tokens** sent through input prompts and returned in output completions.\n",
    "\n",
    "Often these are charged at different rates (usually with higher prices per output token), and we can model these with LLMeter's built-in `InputTokens` and `OutputTokens` dimensions.\n",
    "\n",
    "Let's assume for example you'd like to call the Anthropic Claude 3.5 Haiku model on Amazon Bedrock On-Demand Throughput. Referring to the [Amazon Bedrock pricing page](https://aws.amazon.com/bedrock/pricing/), we can set up a basic cost model as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = BedrockConverseStream(\n",
    "    # Model IDs from the table here:\n",
    "    # https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    ")\n",
    "\n",
    "cost_model = CostModel(\n",
    "    # Request-level costs:\n",
    "    request_dims=[\n",
    "        # At the time of writing, Bedrock pricing page for Claude 3 Haiku *in us-east-1* lists:\n",
    "        # $0.00025 per thousand input tokens, $0.00125 per thousand output tokens\n",
    "        dimensions.InputTokens(price_per_million=0.25),\n",
    "        dimensions.OutputTokens(price_per_million=1.25),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check things are working, we can invoke our endpoint with an example payload and estimate the costs for that specific request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_payloads = [\n",
    "    BedrockConverseStream.create_payload(\n",
    "        \"Tell me a short story about a caterpillar that learns to forgive\",\n",
    "        max_tokens=2000,\n",
    "    ),\n",
    "    BedrockConverseStream.create_payload(\n",
    "        \"In what year did Singapore Changi airport open?\",\n",
    "        max_tokens=200,\n",
    "    ),\n",
    "    BedrockConverseStream.create_payload(\n",
    "        \"When should I use lists vs tuples in Python?\",\n",
    "        max_tokens=1000,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = endpoint.invoke(payload=sample_payloads[1])\n",
    "print(\"--- Response ---\")\n",
    "print(response)\n",
    "\n",
    "print(\"\\n--- Cost Estimate ---\")\n",
    "req_est = await cost_model.calculate_request_cost(response)\n",
    "print(f\"Dimensions: {req_est}\")\n",
    "print(f\"Total: ${req_est.total:f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More interestingly, we can analyze overall and average-per-request costs by passing the model in as a **callback** to a test run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_test = Runner(\n",
    "    endpoint,\n",
    "    output_path=f\"outputs/{endpoint.model_id}\",\n",
    "    callbacks=[cost_model],  # <- Specify our cost model\n",
    ")\n",
    "results = await endpoint_test.run(\n",
    "    payload=sample_payloads,\n",
    "    n_requests=9,\n",
    "    clients=3,\n",
    ")\n",
    "\n",
    "original_total_cost = results.cost_total\n",
    "original_per_req_avg = results.stats.get(\"cost_per_request-average\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On these results:\n",
    "\n",
    "- `cost_InputTokens` is the total estimated charges *for input tokens* in the test Run\n",
    "- `cost_OutputTokens` is the total estimated charges *for output tokens* in the test Run\n",
    "- `cost_total` is the total of all estimated costs for the test Run (should equal `cost_InputTokens + cost_outputTokens` in this case)\n",
    "- You'll also see **statistics** for the total and per-dimension costs at the request level. For example:\n",
    "    - `cost_OutputTokens_per_request-p50` is the *median* output tokens charge per request in the test run\n",
    "    - `cost_per_request_average` is the *mean average* overall cost per request in the test run\n",
    "\n",
    "In this example we deliberately set up some payloads with different expected output lengths, so you should see some variation between the average, `p50`, and `p90` request costs.\n",
    "\n",
    "It's also possible to re-analyze your run Result with a different Cost Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_model_2 = CostModel(\n",
    "    request_dims=[\n",
    "        dimensions.InputTokens(price_per_million=5),\n",
    "        dimensions.OutputTokens(price_per_million=10),\n",
    "    ],\n",
    ")\n",
    "\n",
    "cost_estimate_2 = await cost_model_2.calculate_run_cost(\n",
    "    results,\n",
    "    # Optionally, add `save=True` here to overwrite the estimates and stats on the `results` object\n",
    ")\n",
    "\n",
    "print(\"--- Original estimates ---\")\n",
    "print(f\"Total run cost: ${original_total_cost:f}\")\n",
    "print(f\"Average cost per request: ${original_per_req_avg:f}\")\n",
    "\n",
    "print(\"\\n--- Alternative model ---\")\n",
    "print(f\"Total run cost: ${cost_estimate_2.total:f}\")\n",
    "# Without `save`-ing the estimates to the results, `results.stats` won't be updated. You can still\n",
    "# generate the stats separately as shown below - it's just more complex:\n",
    "new_req_costs = [\n",
    "    await cost_model_2.calculate_request_cost(r) for r in results.responses\n",
    "]\n",
    "new_cost_stats = CalculatedCostWithDimensions.summary_statistics(new_req_costs)\n",
    "print(f\"Average cost per request: ${new_cost_stats.get('total-average')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By associating a cost model with your runs (or analyzing runs with one retrospectively), we've seen how you can explore detailed cost breakdowns; totals; and summary statistics over all the requests in the run.\n",
    "\n",
    "In the next sections, we'll explore more complex scenarios including cost-drivers independent of individual requests - or bringing custom cost dimensions.\n",
    "\n",
    "> ⚠️ **Warning:** The `cost_per_request-average` statistic shown above is the *average of request-level costs*. If you use a Cost Model with *both* request-level and run-level cost dimensions (as described below), you probably want `results.cost_total / results.total_requests` instead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infrastructure-based pricing example (with Amazon SageMaker)\n",
    "\n",
    "For *deployment-based* services like [Amazon Bedrock Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html), self-managed model servers, or [Amazon SageMaker Real-Time Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html), pricing is mainly driven by the amount of time your provisioned endpoint is available - rather than charges per-request.\n",
    "\n",
    "In these cases, we can use LLMeter's **run-level cost dimensions** to model costs that can't be broken down to the individual request level - but may still be useful to analyze at the run level.\n",
    "\n",
    "As an example, let's consider load-testing a single-instance SageMaker real-time inference endpoint to explore how cost, latency, and throughput interact.\n",
    "\n",
    "Assuming we've deployed a smaller LLM (like Mistral 7B or Llama 3.1 8B) from SageMaker JumpStart, to a single instance, we can:\n",
    "\n",
    "- Refer to the [host instance storage volumes table](https://aws.amazon.com/releasenotes/host-instance-storage-volumes-table/) to find the default EBS volume size attached to our chosen instance type\n",
    "- Refer to the [SageMaker pricing page](https://aws.amazon.com/sagemaker/pricing/) to find:\n",
    "  - The price per hour for the instance type deployed (for example, at the time of writing, an `ml.g5.4xlarge` in region `us-east-1` was listed at $2.03/hour)\n",
    "  - The price per month of provisioned SSD storage ($0.14 per GB-month at writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"\"  # TODO: Your deployed SageMaker JumpStart model ID\n",
    "model_version = \"*\"  # TODO: Replace with your specific version, or leave '*' for latest\n",
    "endpoint_name = \"\"  # TODO: Your deployed SageMaker JumpStart endpoint name\n",
    "\n",
    "cost_model = CostModel(\n",
    "    # Run-level costs:\n",
    "    run_dims={\n",
    "        # Note we can provide a dictionary instead of a list, to explicitly name our cost\n",
    "        # dimensions:\n",
    "        \"ComputeHours\": dimensions.EndpointTime(price_per_hour=2.03),\n",
    "        \"EBSStorage\": dimensions.EndpointTime(price_per_hour=0.14 * 600 / (24 * 30)),\n",
    "    }\n",
    ")\n",
    "cost_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, `llmeter.callbacks.cost.providers.sagemaker` provides tools that can attempt to look up these costs automatically based on your endpoint name or deployed instances - but you'll need to have the AWS IAM [`pricing:GetProducts`](https://docs.aws.amazon.com/service-authorization/latest/reference/list_awspricelist.html) (for price lookup) and [`sagemaker:DescribeEndpoint` and `sagemaker:DescribeEndpointConfig`](https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonsagemaker.html) (for endpoint instance type/count lookup) permissions for this to work.\n",
    "\n",
    "If you have the relevant IAM permissions, try running the cell below to set up the automatic cost model and see how it compares to your manually-crafted one above. Otherwise, you can continue with the manual cost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmeter.callbacks.cost.providers.sagemaker import (\n",
    "    cost_model_from_sagemaker_realtime_endpoint,\n",
    ")\n",
    "\n",
    "cost_model_auto = cost_model_from_sagemaker_realtime_endpoint(endpoint_name)\n",
    "cost_model_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll try configuring the LLMeter endpoint and fetching the JumpStart-provided example payloads from the parameters you provided above. If you find errors, check your deployed model ID, version and endpoint type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "endpoint = SageMakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_id=model_id,\n",
    "    # See 'LLMeter with Amazon SageMaker JumpStart.ipynb' for tips on checking this value for your\n",
    "    # model:\n",
    "    generated_text_jmespath=\"generated_text\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Fetching sample payloads for JumpStart model {model_id}, version {model_version}\"\n",
    ")\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "sample_payloads = [k.body for k in (model.retrieve_all_examples() or []) if k.body]\n",
    "print(f\"Got {len(sample_payloads)} sample payloads\")\n",
    "\n",
    "print(\"Testing endpoint with first sample payload\")\n",
    "print(endpoint.invoke(sample_payloads[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the endpoint set up and a cost model defined, we can run a load test and attach the cost model as a callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_test = LoadTest(\n",
    "    endpoint=endpoint,\n",
    "    payload=sample_payloads,\n",
    "    sequence_of_clients=[1, 5, 20, 50, 100, 500],\n",
    "    min_requests_per_client=5,\n",
    "    min_requests_per_run=20,\n",
    "    output_path=f\"outputs/{endpoint.model_id}/sweep\",\n",
    "    callbacks=[cost_model],  # <- Include the cost model as callback\n",
    ")\n",
    "sweep_results = await sweep_test.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a `LoadTest`, a `Run`/`Result` is created for each concurrency level specified in `sequence_of_clients` - and each should now be annotated with cost estimates.\n",
    "\n",
    "In this model, our only cost contributors are driven by the overall time the endpoint is provisioned. We can compare the latency, error rate, successful requests-per-second throughput, and estimated cost per successful request - to understand the trade-offs of different request concurrencies to this deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in sweep_results:\n",
    "    print(f\"{result.clients} concurrent clients:\")\n",
    "    successful_requests = result.total_requests - result.stats[\"failed_requests\"]\n",
    "    print(f\"  - Request error rate {result.stats.get('failed_requests_rate'):.2%}\")\n",
    "    print(\n",
    "        f\"  - Avg latency {result.stats.get('time_to_last_token-average') * 1000:.0f}ms\"\n",
    "    )\n",
    "    print(f\"  - p90 latency {result.stats.get('time_to_last_token-p90') * 1000:.0f}ms\")\n",
    "    print(\n",
    "        f\"  - {successful_requests / (result.end_time - result.start_time):.2f} reqs/sec\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  - Est. ${result.cost_total / successful_requests:f} per successful request\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, we can understand both the quality of service and cost-to-serve offered by this deployment as a function of concurrent request volume. This can help us decide how many instances to deploy based on our expected production request rate.\n",
    "\n",
    "Alternatively, you could run load tests (or single test Runs) with different instance types to explore the best available trade-off for a single-instance endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing custom cost dimensions\n",
    "\n",
    "`llmeter.callbacks.cost.dimensions` provides a few pre-built cost dimensions to support the most common pricing types (and if others would be useful for you, please raise an issue and/or pull request!) ...But what if you need to model something different?\n",
    "\n",
    "You can define your own cost dimensions either at request- or run-level (or both), and the easiest way is to inherit from the `RequestCostDimensionBase` and `RunCostDimensionBase` classes.\n",
    "\n",
    "Request cost dimensions must implement `calculate(...)` to calculate a **request**'s cost based on the initial LLMeter `InvocationResponse` (which includes metadata like the numbers of input & output tokens, the latency, and etc). For example, maybe you're using a service like AWS Lambda which charges based on the actual run-time of each function call. You could define a dimension something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RequestRunTime(dimensions.RequestCostDimensionBase):\n",
    "    price_per_millisecond: float\n",
    "\n",
    "    async def calculate(self, response: InvocationResponse) -> float:\n",
    "        return response.time_to_last_token * self.price_per_millisecond * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cost dimensions must implement `calculate(...)` to calculate a **run**'s cost based on the initial LLMeter `Result`.\n",
    "\n",
    "They can also **optionally** implement `before_run_start(...)`, which receives the initial `Runner`, in case you need to set up any initial state to support cost monitoring.\n",
    "\n",
    "For example, maybe you're using a service that charges by \"session\" and running each Run as a separate session in the API. You might define a dimension like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FlatChargePerRun(dimensions.RunCostDimensionBase):\n",
    "    price: float\n",
    "\n",
    "    async def calculate(self, result: Result) -> float:\n",
    "        return self.price if len(result.responses) > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These dimensions can be included in cost models just like the built-in `InputTokens`, `OutputTokens`, and `EndpointTime` we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdm_cost_model = CostModel(\n",
    "    request_dims=[RequestRunTime(price_per_millisecond=0.01)],\n",
    "    run_dims=[FlatChargePerRun(price=2)],\n",
    ")\n",
    "\n",
    "cdm_runner = Runner(\n",
    "    endpoint,\n",
    "    output_path=f\"outputs/{endpoint.model_id}\",\n",
    "    callbacks=[cdm_cost_model],  # <- Specify our cost model\n",
    ")\n",
    "cdm_result = await cdm_runner.run(\n",
    "    payload=sample_payloads,\n",
    "    n_requests=9,\n",
    "    clients=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember** with a composite cost model like this one including both run-level and request-level dimensions, that \"per-request\" statistics will **exclude** any overall run-level costs.\n",
    "\n",
    "In this example:\n",
    "\n",
    "- `cost_total` includes both dimensions\n",
    "- `cost_per_request-average` **only** includes the `RequestRunTime` dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in cdm_result.stats.items() if k.startswith(\"cost_\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Cost is important, but complex to analyze. From tiered pricing to enterprise agreements to shared infrastructure and resources, a wide range of factors can complicate analysis and comparison.\n",
    "\n",
    "LLMeter provides flexible tooling for you to define the cost components that are important for your use-case, and draw comparisons that make sense in your context.\n",
    "\n",
    "While this can include comparing models with very different pricing structures (such as token-based pricing vs infrastructure-based pricing), it's important to remember that such a comparison can only make sense based on an expected workload. With LLMeter, you can run a range of tests with different datasets and concurrency rates, to explore different scenarios and the actual trade-offs observed between them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
