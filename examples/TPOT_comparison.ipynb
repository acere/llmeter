{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing time per output token\n",
    "\n",
    "In this notebook we load the result of two runs and compare their behavior with respect to time per output token (TPOT from now on).  \n",
    "This analysis generalizes well also in cases of dissimilar workloads. Ideally (and excluding non-linear approaches like speculative decoding), the time per output token should be a property of the underlying accelerated compute, and it should be independent of the number of input and generated tokens.\n",
    "\n",
    "For the analysis we'll leverage some of the plotting functions provided by LLMeter. These functions uses Plotly, and can be combined to create custom visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "from llmeter.plotting import (\n",
    "    boxplot_by_dimension,\n",
    "    histogram_by_dimension,\n",
    "    scatter_histogram_2d,\n",
    ")\n",
    "from llmeter.results import Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the plotly template for the rest of the notebook to `plotly_white`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load two datasets. As mentioned in the introduction, this analysis will only make sense if the 2 runs are compatible, that is the number of input tokens is the same, or close to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = Result.load(\"<path of saved results of first run>\")\n",
    "result_2 = Result.load(\"<path of saved results of second run>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = \"time_per_output_token\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT vs num of output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = scatter_histogram_2d(result_1, \"num_tokens_output\", dimension, 20, 20)\n",
    "fig.update_layout(title=result_1.run_name)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = scatter_histogram_2d(result_2, \"num_tokens_output\", dimension, 20, 20)\n",
    "fig.update_layout(title=result_2.run_name)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution comparison\n",
    "\n",
    "It might be interesting to have a better understanding of the actual distribution of the TPOT, for example by observing the distribution using boxplots or histograms. We'll start by creating a boxplot for each run using , and then combining them to provide a clear comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "tr1 = boxplot_by_dimension(result=result_1, dimension=dimension)\n",
    "tr2 = boxplot_by_dimension(result=result_2, dimension=dimension)\n",
    "fig.add_traces(\n",
    "    [\n",
    "        tr1,\n",
    "        tr2,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# use log scale for the time axis\n",
    "fig.update_xaxes(type=\"log\")\n",
    "\n",
    "fig.update_layout(\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    "    title=f\"Comparison of {dimension.replace('_', ' ').capitalize()}\",\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create an histogram to visualize the two distributions using `histogram_by_dimension()`. This function is based on plotly `go.Histogram()`, and accepts all the modifier keywords arguments. In this case, we define the size of the histogram bin to be 10 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbins = dict(size=0.001)\n",
    "\n",
    "fig = go.Figure()\n",
    "h1 = histogram_by_dimension(\n",
    "    result_1,\n",
    "    dimension,\n",
    "    # xbins=xbins,\n",
    ")\n",
    "h2 = histogram_by_dimension(\n",
    "    result_2,\n",
    "    dimension,\n",
    "    xbins=xbins,\n",
    ")\n",
    "\n",
    "fig.add_traces([h1, h2])\n",
    "\n",
    "fig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the difference of the median values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is potentially highly skewed. Assuming there's enough representative data points, we'll use bootstrapping to estimate confidence intervals on the statistics of interest, in this case the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets without any Null value\n",
    "data_1 = [k for k in result_1.get_dimension(dimension) if k]\n",
    "data_2 = [k for k in result_2.get_dimension(dimension) if k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = bootstrap((data_1,), np.median, confidence_level=0.95)\n",
    "print(\n",
    "    f\"Median of {dimension} for {result_1.run_name}\\n \"\n",
    "    f\"{np.median(data_1):.3g} ({res.confidence_interval.low:.3g}, {res.confidence_interval.high:.3g})s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = bootstrap((data_2,), np.median, confidence_level=0.95)\n",
    "print(\n",
    "    f\"Median of {dimension} for {result_2.run_name}\\n\"\n",
    "    f\"{np.median(data_2):.3g} ({res.confidence_interval.low:.3g}, {res.confidence_interval.high:.3g})s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_f(sample1, sample2, axis=-1):\n",
    "    median_1 = np.median(sample1, axis=axis)\n",
    "    median_2 = np.median(sample2, axis=axis)\n",
    "    return median_2 - median_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (data_1, data_2)\n",
    "\n",
    "res = bootstrap((data), obj_f, confidence_level=0.95)\n",
    "\n",
    "print(\n",
    "    f\"Difference between median {dimension} for {result_1.run_name} and {result_2.run_name} is\\n\"\n",
    "    f\"{obj_f(data[0], data[1]):.3g} ({res.confidence_interval.low:.3g}, {res.confidence_interval.high:.3g})s\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
