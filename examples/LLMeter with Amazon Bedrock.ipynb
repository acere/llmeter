{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMeter with Amazon Bedrock\n",
    "\n",
    "This notebook demonstrates how you can use LLMeter to measure latency of LLMs hosted on [Amazon Bedrock](https://aws.amazon.com/bedrock/), for comparing between available models or understanding the impact of your workload's prompt (input) and completion (output) lengths on response time.\n",
    "\n",
    "We start with the basics of invoking LLMs through LLMeter and running request batches - then discuss the higher-level *experiment* offered by LLMeter for length-based latency heatmapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "First, ensure your environment has LLMeter installed (and its optional `plotting` utilities) ready to run the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"llmeter[plotting]<1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Dependencies:\n",
    "from upath import UPath  # Combined APIs for accessing cloud or local storage\n",
    "\n",
    "from llmeter.endpoints import BedrockConverse, BedrockConverseStream\n",
    "from llmeter.runner import Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes you're running in an environment with [configured AWS API credentials](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) (e.g. through environment variables, profiles, or similar), and that the configured identity has [permissions](https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html) to `bedrock:InvokeModel` and `bedrock:InvokeModelWithResponseStream`. You'll also need to [enable access](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html) to your chosen foundation model(s), as this is not granted by default.\n",
    "\n",
    "LLMeter's `BedrockConverse` and `BedrockConverseStream` endpoints for Amazon Bedrock use the [Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).\n",
    "\n",
    "Refer to the [model IDs table](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html) and [list of models that support the Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html) this API, to configure an ID below for a supported model you'd like to evaluate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"model_id\"  # <-- Choose a Bedrock model ID from the above-linked list that supports Converse API\n",
    "\n",
    "if not model_id:\n",
    "    raise ValueError(\"Please set a valid model ID!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes, we can generate a sample payload matching the [Bedrock Converse API format](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) using a convenience method of the LLMeter `BedrockConverse` class as shown below.\n",
    "\n",
    "For the full Converse API specification, you can check the [documentation here](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_payload = BedrockConverse.create_payload(\n",
    "    \"Create a list of 3 pop songs.\",\n",
    "    max_tokens=512,\n",
    "    system=[{\"text\": \"you're an expert in pop and indie music\"}],\n",
    ")\n",
    "sample_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic inference and test runs\n",
    "\n",
    "To measure the latency and throughput of the model (including the network latency from the client where LLMeter is running to the actual endpoint), we start by creating a [`llmeter.Endpoint`](../llmeter/endpoints/base.py) object specifically for Bedrock deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLMeter Endpoint object for your chosen Bedrock model:\n",
    "bedrock_endpoint = BedrockConverse(\n",
    "    model_id=model_id,\n",
    "    # Optionally, you can target a different AWS Region than your default:\n",
    "    # region=\"us-west-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now verify that LLMeter is correctly capturing the output of the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_endpoint.invoke(payload=sample_payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that `time_to_last_token` (the overall response time), `num_tokens_input` and `num_tokens_output` (the actual consumed input and output token counts) are captured in the response. `time_to_first_token` is null in this case because it's only used in streaming APIs (see next section).\n",
    "\n",
    "Of course a single data point give us much confidence about typical endpoint performance.\n",
    "\n",
    "We can use the [`Runner`](../llmeter/runner.py) class to set up a basic test run and calculate statistics.\n",
    "- `clients` configures the number of concurrent (thread-based) clients that will send requests to the endpoint.\n",
    "- `n_requests` is the total number of consecutive requests **each client** should perform (so the endpoint will receive clients * n_requests requests in total).\n",
    "- `payload`s are the sample request(s) that should be used for the test. If `n_requests=None`, each client will just iterate through this list. Otherwise, the list will be shuffled to generate the required number of requests.\n",
    "- `output_path` can be a local path or `s3://...` URI, where the test results should be saved.\n",
    "\n",
    "The `run()` method is an async function, and to execute it in a Jupyter notebook it requires to use the `await` keyword!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_test = Runner(\n",
    "    bedrock_endpoint,\n",
    "    output_path=f\"outputs/{bedrock_endpoint.model_id}\",\n",
    ")\n",
    "results = await endpoint_test.run(payload=sample_payload, n_requests=3, clients=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the test is completed, we can check the overall statistics aggregated from all the clients and requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also drill down to the individual responses if needed, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `output_path` was set, we can also find this information saved to files in the provided location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Contents of {results.output_path}\\n----\")\n",
    "for sub in UPath(results.output_path).iterdir():\n",
    "    print(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming responses and Time-to-First-Token\n",
    "\n",
    "Many LLM providers support *streaming* APIs which reduce user-perceived latency by sending the response in chunks rather than waiting for the whole generation to finish. In these contexts, the `time_to_first_token` is also a useful metric to understand how long consumers will have to wait before they can start to process the response.\n",
    "\n",
    "To use Bedrock's streaming API, we can instead connect with an LLMeter `BedrockConverseStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_endpoint_stream = BedrockConverseStream(\n",
    "    model_id=model_id,\n",
    "    # As before, you could target a different AWS Region than your default:\n",
    "    # region=\"us-west-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can first test that the `Endpoint` is correctly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_endpoint_stream.invoke(payload=sample_payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're satisfied that the `Endpoint` is working as expected and we're now able to capture the `time_to_first_token` metric on the response - we can set up out test runner and `run()` it, as we did for the non-streaming case above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_stream_test = Runner(\n",
    "    bedrock_endpoint_stream,\n",
    "    output_path=f\"outputs/{bedrock_endpoint_stream.model_id}\",\n",
    ")\n",
    "results_stream = await endpoint_stream_test.run(\n",
    "    payload=sample_payload, clients=20, n_requests=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the summary statistics and individual responses as before, but now have a additional statistics to work with:\n",
    "\n",
    "> ℹ️ **Note:** The `time_to_last_token` may differ between streaming and non-streaming invocations, so it's important to test the invocation method you'll actually be using. Don't choose to test streaming only because it provides additional statistics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_stream.responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've seen the core mechanics of running tests with LLMeter, let's explore the result using plots.  \n",
    "We import few functions that help us generate [Plotly](https://plotly.com/python/) traces and figures. Traces are especially convenient because they can easily be combined. You can see examples of this in other example notebooks dedicated to the comparison of endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmeter.plotting import (\n",
    "    boxplot_by_dimension,\n",
    "    histogram_by_dimension,\n",
    "    scatter_histogram_2d,\n",
    ")\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first plot we generate is an analysis of the distribution of input and output prompts in out test. In this case, we used the same prompt as input for all the invocations, so the number of input prompts is constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_histogram_2d(\n",
    "    results_stream,\n",
    "    x_dimension=\"num_tokens_input\",\n",
    "    y_dimension=\"num_tokens_output\",\n",
    "    n_bins_x=20,\n",
    "    n_bins_y=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of constant input prompt, it makes sense to look at the distribution of the time to first token. We generate the histogram trace with `histogram_by_dimension()`, and add it to a figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout=dict(title=\"Time to first token - histogram\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    histogram_by_dimension(\n",
    "        results_stream, dimension=\"time_to_first_token\", xbins=dict(size=0.02)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same statistics can be summarized into a boxplot. This is particularly powerful when comparing multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout=dict(title=\"Time to first token - boxplot\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    boxplot_by_dimension(\n",
    "        results_stream, dimension=\"time_to_first_token\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the relationship between the _time per output token_ (TPOT) and the number of output token. We expect this to be independent, but that might not be the case when the token count is derived using a tokenizer different than the model's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_histogram_2d(\n",
    "    results_stream,\n",
    "    x_dimension=\"num_tokens_output\",\n",
    "    y_dimension=\"time_per_output_token\",\n",
    "    n_bins_x=20,\n",
    "    n_bins_y=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on load testing\n",
    "\n",
    "If you're deploying an LLM on a service like Amazon SageMaker where you **size the underlying infrastructure** (number of compute instances, GPUs, etc) - then of course it will be important to understand how the latency of your deployed model server(s) varies depending on the number of concurrent users / requests.\n",
    "\n",
    "However, if you're using a large-scale, as-a-service LLM provider like [Amazon Bedrock](https://aws.amazon.com/bedrock/) - it's likely that the request volume for your use-case is insignificant compared to what the service handles overall. In this context, rather than running actual load/volume tests it's more relevant to **check your [API quotas](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html)** are sufficient for your planned workload - and request increases if necessary.\n",
    "\n",
    "LLMeter does provide a `LoadTest` experiment to help measure how latency and throughput change as a function of the number of concurrent clients / requests, but for the above reasons we won't cover it in this example. Check the [SageMaker JumpStart example notebook](./LLMeter%20with%20Amazon%20SageMaker%20JumpStart.ipynb) for example code, which should work just fine with Bedrock endpoints too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
